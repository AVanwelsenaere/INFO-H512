{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42e5235-76fc-40b4-899e-76e8020a078d",
   "metadata": {},
   "source": [
    "# Scanned PDF Acquisition and Cleaning Notebook\n",
    "## How to use this notebook ?\n",
    "\n",
    "Enter the connect string and the PDF path and Run All Cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e1e4b-eb46-44f5-8fbc-4258e06ad235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECLARATIONS & CONSTANTS      #\n",
    "#################################\n",
    "\n",
    "# Requirements : \n",
    "# 1. A working installation of Tesseract with the french package ('fra').\n",
    "# 2. A working installation of Ollama with the selected model downloaded.\n",
    "# 3. A SQL Server up and running with the right database.\n",
    "\n",
    "#pip install pymupdf\n",
    "#pip install pymongo\n",
    "#pip install pdfminer.six\n",
    "#pip install pytesseract\n",
    "#pip install opencv-python\n",
    "#pip install pyspellchecker\n",
    "#pip install ollama\n",
    "\n",
    "import numpy as oNumPy\n",
    "import pandas as oPandas\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "import pyodbc\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "from pdfminer.high_level import extract_text\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import ollama\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from difflib import get_close_matches\n",
    "\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "\n",
    "############# SOURCE PDF ############################################\n",
    "#_sPDF_Path = \"Coléoptères Carabiques - Subset_Mistral.pdf\"\n",
    "\n",
    "_sPDF_Path = \"British Spiders_Alexandra.pdf\"\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "_sSave_Mode = \"SQL\"                   # Options: \"SQL\" or \"FILES\"\n",
    "_sBase_Output_Dir = \"ExtractedBooks\"  # Root folder for file-based saving\n",
    "_bUse_OCR = True                      # Toggle usage of Tesseract\n",
    "_bUse_Ollama = True                   # Toggle usage of Ollama AI\n",
    "_bUse_Species_Index = True            # Toggle usage of the Excel file with taxonomy\n",
    "_sTaxonomy_Filename = \"Species_Index.xlsx\"\n",
    "_sTarget_Language = \"français\"        # Or \"english\", \"dutch\", etc.\n",
    "\n",
    "_sConnectString = \"DRIVER={SQL Server};SERVER=ROCINANTE;DATABASE=Digital_Library;UID=XXX;PWD=XXX;\"\n",
    "_sOllamaModel = \"mistral\"             # Models : deepseek-r1:32b, deepseek-r1:8b, mistral #\n",
    "\n",
    "\n",
    "# TAXONOMY => Table of contents family ranges\n",
    "family_ranges = {\n",
    "    \"Agelenidae\": range(1, 32),\n",
    "    \"Mimetidae\": range(32, 37),\n",
    "    \"Theridiidae\": range(37, 92),\n",
    "    \"Nesticidae\": range(92, 94),\n",
    "    \"Tetragnathidae\": range(94, 111),\n",
    "    \"Argiopidae\": range(111, 172),\n",
    "    \"Linyphiidae\": range(172, 418)\n",
    "}\n",
    "\n",
    "# TAXONOMY => Headers\n",
    "section_headers = {\n",
    "    \"DESCRIPTION\": \"DESCRIPTION\",\n",
    "    \"OCCURRENCE\": \"OCCURRENCE\",\n",
    "    \"CHARACTERS OF GENUS\": \"GENUS_DESCRIPTION\",\n",
    "    \"CHARACTERS OF FAMILY\": \"FAMILY_DESCRIPTION\",\n",
    "    \"KEY TO THE GENERA\": \"GENUS_KEY\",\n",
    "    \"NOTES\": \"NOTES\",\n",
    "    \"REMARKS\": \"NOTES\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c194be2f-1227-4f02-9b1a-46d9448513d6",
   "metadata": {},
   "source": [
    "# OCR & Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d518bb44-9dbe-4b07-a118-7b234fa59935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR of a book (or PDF Document) #\n",
    "###################################\n",
    "\n",
    "def Is_Text_PDF(sPDF_Path):\n",
    "    # Check if the PDF is in text or image mode.\n",
    "    sText = extract_text(sPDF_Path)\n",
    "    return len(sText.strip()) > 0\n",
    "\n",
    "\n",
    "def Clean_Text(sText):\n",
    "    sText = sText.replace(\"\\n\", \" \").strip()\n",
    "    return ' '.join(sText.split())\n",
    "\n",
    "\n",
    "def Clean_Text_Advanced(sText):   \n",
    "    if not sText:\n",
    "        return \"\"\n",
    "\n",
    "    sText = sText.replace(\"\\n\", \" \").strip()\n",
    "    sText = re.sub(r'\\s+', ' ', sText)  # Clean long spaces\n",
    "    sText = re.sub(r'[^a-zA-ZÀ-ÿ0-9,.!? ]+', '', sText)  \n",
    "    \n",
    "    return sText\n",
    "\n",
    "\n",
    "def Correct_Text_with_Ollama(sModel, sText, sPrompt):\n",
    "\n",
    "    if not sText or sText.strip() == \"\":\n",
    "        return sText\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(model=sModel, messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"{sPrompt} {sText}\"}\n",
    "        ])\n",
    "        \n",
    "        return response[\"message\"][\"content\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error with Ollama : {e}\")\n",
    "        return sText\n",
    "\n",
    "\n",
    "# === Extract text with formatting => Usage with Taxonomy ===\n",
    "def extract_text_with_formatting(page):\n",
    "    text = \"\"\n",
    "    blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "\n",
    "    for block in blocks:\n",
    "        if \"lines\" not in block:\n",
    "            continue\n",
    "        for line in block[\"lines\"]:\n",
    "            line_text = \"\"\n",
    "            for span in line[\"spans\"]:\n",
    "                span_text = span[\"text\"]\n",
    "                font = span[\"font\"].lower()\n",
    "                if \"bold\" in font:\n",
    "                    span_text = f\"<b>{span_text}</b>\"\n",
    "                if \"italic\" in font:\n",
    "                    span_text = f\"<i>{span_text}</i>\"\n",
    "                line_text += span_text\n",
    "            text += line_text.strip() + \"\\n\"\n",
    "        text += \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def Extract_Drawings_from_Page(oImage):\n",
    "    # Detects and extracts illustrations from a PDF page using an improved method. \n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(oNumPy.array(oImage), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Apply slight blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Adaptive thresholding to better isolate illustrations\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "    # Contour detection\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # List of extracted drawings\n",
    "    drawings = []\n",
    "    min_size = 5000  # Minimum size to consider an illustration\n",
    "\n",
    "    # Filter contours\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "\n",
    "        # Ignore very small elements\n",
    "        if w * h > min_size:\n",
    "            cropped = oImage.crop((x, y, x + w, y + h))\n",
    "\n",
    "            # Convert to base64 for storage\n",
    "            img_byte_arr = io.BytesIO()\n",
    "            cropped.save(img_byte_arr, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(img_byte_arr.getvalue()).decode(\"utf-8\")\n",
    "            drawings.append(img_base64)\n",
    "\n",
    "    return drawings\n",
    "\n",
    "\n",
    "def Extract_Text_and_Images_from_Page(oPage, bIsTextPDF):\n",
    "    # Extracts text and illustrations from a PDF page. \n",
    "    \n",
    "    # Convert the page to an image\n",
    "    pix = oPage.get_pixmap()\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "    # OCR to retrieve the text (if necessary)\n",
    "    if _bUse_OCR:\n",
    "        sOCRText = Clean_Text(pytesseract.image_to_string(img, lang=\"fra\"))\n",
    "    else:\n",
    "        sOCRText = Clean_Text(oPage.get_text(\"text\"))\n",
    "\n",
    "    oImages = Extract_Drawings_from_Page(img)\n",
    "\n",
    "    # If it's a text-based PDF, get the text directly, otherwise use OCR\n",
    "    if bIsTextPDF:\n",
    "        sText = Clean_Text(oPage.get_text(\"text\"))\n",
    "    else:\n",
    "        sText = sOCRText\n",
    "\n",
    "    return sText, sOCRText, oImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85022f75-38f3-4e6f-8640-d56115c84274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxonomy Functions\n",
    "\n",
    "def fuzzy_match_one(text, choices, threshold=85):\n",
    "    result = process.extractOne(text, choices, score_cutoff=threshold)\n",
    "    return result[0] if result else None\n",
    "\n",
    "\n",
    "def clean_tags(text):\n",
    "    return re.sub(r\"</?[^>]+>\", \"\", text)\n",
    "\n",
    "\n",
    "def normalize_paragraph(text):\n",
    "    text = re.sub(r\"[\\n\\r]+\", \" \", text)\n",
    "    text = re.sub(r\"[\\u2010\\u2011\\u2012\\u2013\\u2014\\u2212\\-]+\", \"-\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def expected_family_for_page(page):\n",
    "    for fam, pages in family_ranges.items():\n",
    "        if page in pages:\n",
    "            return fam\n",
    "    return None\n",
    "\n",
    "\n",
    "# === Taxon matching logic ===\n",
    "def match_taxa(text, family_df, current_genus=None):\n",
    "    text = re.sub(r\"[\\n\\r]+\", \" \", text)\n",
    "    text = re.sub(r\"[\\u2010\\u2011\\u2012\\u2013\\u2014\\u2212\\-]+\", \"-\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    matched_species = []\n",
    "    matched_genera = []\n",
    "\n",
    "    full_species = family_df[\"latin_name\"].tolist()\n",
    "    epithets = family_df[\"species\"].tolist()\n",
    "    genus_choices = family_df[\"genus\"].unique().tolist()\n",
    "\n",
    "    # === Exact full species match\n",
    "    for name in full_species:\n",
    "        if name in text:\n",
    "            row = family_df[family_df[\"latin_name\"] == name].iloc[0]\n",
    "            matched_species.append({\n",
    "                \"name\": name,\n",
    "                \"genus\": row[\"genus\"],\n",
    "                \"epithet\": row[\"species\"],\n",
    "                \"confidence\": 100,\n",
    "                \"match_type\": \"exact\"\n",
    "            })\n",
    "\n",
    "    # === Epithet + current genus\n",
    "    for epithet in epithets:\n",
    "        if f\" {epithet}\" in text or text.startswith(epithet):\n",
    "            candidates = family_df[family_df[\"species\"] == epithet]\n",
    "            if current_genus:\n",
    "                match = candidates[candidates[\"genus\"] == current_genus]\n",
    "                if not match.empty:\n",
    "                    row = match.iloc[0]\n",
    "                    matched_species.append({\n",
    "                        \"name\": row[\"latin_name\"],\n",
    "                        \"genus\": row[\"genus\"],\n",
    "                        \"epithet\": row[\"species\"],\n",
    "                        \"confidence\": 95,\n",
    "                        \"match_type\": \"epithet + current genus\"\n",
    "                    })\n",
    "            elif len(candidates[\"genus\"].unique()) == 1:\n",
    "                row = candidates.iloc[0]\n",
    "                matched_species.append({\n",
    "                    \"name\": row[\"latin_name\"],\n",
    "                    \"genus\": row[\"genus\"],\n",
    "                    \"epithet\": row[\"species\"],\n",
    "                    \"confidence\": 90,\n",
    "                    \"match_type\": \"unique epithet\"\n",
    "                })\n",
    "\n",
    "    # === Fuzzy epithet match\n",
    "    for epithet in epithets:\n",
    "        if not any(s[\"epithet\"] == epithet for s in matched_species):\n",
    "            score = fuzz.partial_ratio(epithet, text)\n",
    "            if score >= 85:\n",
    "                row = family_df[family_df[\"species\"] == epithet].iloc[0]\n",
    "                matched_species.append({\n",
    "                    \"name\": row[\"latin_name\"],\n",
    "                    \"genus\": row[\"genus\"],\n",
    "                    \"epithet\": row[\"species\"],\n",
    "                    \"confidence\": score,\n",
    "                    \"match_type\": \"fuzzy epithet\"\n",
    "                })\n",
    "\n",
    "    # === Genus block\n",
    "    genus_block = re.match(r\"^(?:\\d+\\.\\s*)?genus\\s+([A-Z][a-z]+)\", text, re.IGNORECASE)\n",
    "    if genus_block:\n",
    "        candidate = genus_block.group(1)\n",
    "        if candidate in genus_choices:\n",
    "            matched_genera.append({\n",
    "                \"name\": candidate,\n",
    "                \"confidence\": 100,\n",
    "                \"source\": \"genus block\"\n",
    "            })\n",
    "        else:\n",
    "            fuzzy = process.extractOne(candidate, genus_choices, score_cutoff=85)\n",
    "            if fuzzy:\n",
    "                matched_genera.append({\n",
    "                    \"name\": fuzzy[0],\n",
    "                    \"confidence\": fuzzy[1],\n",
    "                    \"source\": \"genus block (fuzzy)\"\n",
    "                })\n",
    "\n",
    "    # === Add genus from species match\n",
    "    for s in matched_species:\n",
    "        if not any(g[\"name\"] == s[\"genus\"] for g in matched_genera):\n",
    "            matched_genera.append({\n",
    "                \"name\": s[\"genus\"],\n",
    "                \"confidence\": s[\"confidence\"],\n",
    "                \"source\": \"from species\"\n",
    "            })\n",
    "\n",
    "    return matched_genera, matched_species\n",
    "\n",
    "\n",
    "# === Flatten species and genera for one-row-per-page ===\n",
    "def join_matches(match_list, key):\n",
    "    return \", \".join(str(m[key]) for m in match_list if isinstance(m, dict))\n",
    "\n",
    "\n",
    "def Load_Species_Index():\n",
    "    # Load reference Excel\n",
    "    excel_path = os.path.join(\"\", _sTaxonomy_Filename)\n",
    "    df_species = oPandas.read_excel(excel_path).fillna(\"\").astype(str)\n",
    "    \n",
    "    df_species[\"order\"] = df_species[\"order\"].str.strip()\n",
    "    df_species[\"order_common\"] = df_species[\"order_common\"].str.strip()\n",
    "    df_species[\"latin_name\"] = df_species[\"latin_name\"].str.strip()\n",
    "    df_species[\"genus\"] = df_species[\"genus\"].str.strip()\n",
    "    df_species[\"family\"] = df_species[\"family\"].str.strip()\n",
    "    df_species[\"species\"] = df_species[\"species\"].str.strip()\n",
    "    df_species[\"short_name\"] = df_species.apply(\n",
    "        lambda row: f\"{row['genus'][0]}. {row['species']}\" if row[\"genus\"] and row[\"species\"] else \"\", axis=1\n",
    "    )\n",
    "    return df_species\n",
    "\n",
    "\n",
    "def Extract_Species_From_Page(sText_Ollama, iPage_Num, df_species, family_ranges):\n",
    "    \"\"\"\n",
    "    :param sText_Ollama: Texte brut de la page (str)\n",
    "    :param iPage_Num: Numéro de la page (int)\n",
    "    :param df_species: DataFrame de taxonomie (colonnes: family, genus, species, latin_name, etc.)\n",
    "    :param family_ranges: Dictionnaire des familles par plage de pages\n",
    "    :return: (dict) contenant les genres et espèces extraits\n",
    "    \"\"\"\n",
    "\n",
    "    family = expected_family_for_page(iPage_Num)\n",
    "    if not family:\n",
    "        return None\n",
    "    \n",
    "    family_df = df_species[df_species[\"family\"] == family]\n",
    "    genera, species = match_taxa(sText_Ollama, family_df)\n",
    "    \n",
    "    ordre = family_df[\"order\"].iloc[0] if \"order\" in family_df.columns else \"\"\n",
    "    ordre_common = family_df[\"order_common\"].iloc[0] if \"order_common\" in family_df.columns else \"\"\n",
    "    \n",
    "    return {\n",
    "        \"page\": iPage_Num,\n",
    "        \"order\": ordre,\n",
    "        \"order_common\": ordre_common,\n",
    "        \"family\": family,\n",
    "        \"genus\": \", \".join(g[\"name\"] for g in genera),\n",
    "        \"genus_conf\": \", \".join(str(g[\"confidence\"]) for g in genera),\n",
    "        \"genus_source\": \", \".join(g[\"source\"] for g in genera),\n",
    "        \"species\": \", \".join(s[\"name\"] for s in species),\n",
    "        \"species_conf\": \", \".join(str(s[\"confidence\"]) for s in species),\n",
    "        \"species_type\": \", \".join(s[\"match_type\"] for s in species),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2817b7-5f4c-4247-ae3d-ecc93f19fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN PROCESS DEFINITION            #\n",
    "######################################\n",
    "\n",
    "def Do_Process_PDF(sPDF_Path):\n",
    "    try:\n",
    "        oDoc = fitz.open(sPDF_Path)\n",
    "        file_size = os.path.getsize(sPDF_Path) / (1024 * 1024)  # Size in MB\n",
    "\n",
    "        print(f\"File Size : {file_size:.2f} MB\")\n",
    "        print(f\"Page(s) : {len(oDoc)}\")\n",
    "\n",
    "        sBook_Title = os.path.splitext(os.path.basename(sPDF_Path))[0]  # PDF Name without extension\n",
    "\n",
    "        if (_bUse_Species_Index):\n",
    "            df_species = Load_Species_Index()\n",
    "        \n",
    "        if _sSave_Mode == \"SQL\":\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO Books (book_title, book_date_added, book_pages_count)\n",
    "                OUTPUT INSERTED.book_id_pkey\n",
    "                VALUES (?, GETDATE(), ?)\n",
    "            \"\"\", (sBook_Title, len(oDoc)))\n",
    "            iBook_ID = cursor.fetchone()[0]\n",
    "            conn.commit()\n",
    "        else:\n",
    "            output_dir = os.path.join(_sBase_Output_Dir, sBook_Title)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for iPage_Num, oPage in enumerate(oDoc):\n",
    "            dPage_Start_Time = time.time()\n",
    "\n",
    "            print(f\"Page {iPage_Num+1} : Begin OCR and Image extraction.\")\n",
    "\n",
    "            sText, sOCRText, oImages = Extract_Text_and_Images_from_Page(oPage, Is_Text_PDF(sPDF_Path))\n",
    "\n",
    "            if (_bUse_Species_Index):\n",
    "                sText = extract_text_with_formatting(oPage)\n",
    "            \n",
    "            sTextClean = Clean_Text_Advanced(sText)\n",
    "\n",
    "            print(f\"Page {iPage_Num+1} : Begin AI enhancement step.\")\n",
    "\n",
    "            sText_Ollama = \"\"\n",
    "            sText_OllamaExp = \"\"\n",
    "            if _bUse_Ollama:\n",
    "                if (_bUse_Species_Index):\n",
    "                    prompt_clean = (f\"\"\"\n",
    "                                You are processing pages from a naturalist book for use in a searchable chatbot.\n",
    "                                \n",
    "                                - Remove visual artifacts like underscores, broken punctuation, pipes, or fake bullets\n",
    "                                - Remove HTML-like formatting tags such as <b>, <i>, <u>, etc\n",
    "                                - Join broken lines and sentences into paragraphs\n",
    "                                - Keep scientific names and taxonomic terms exactly as written\n",
    "                                - Keep original capitalization, especially for headers and scientific terms\n",
    "                                - Only return the cleaned text content. Do not explain your actions\n",
    "                                - Do not add comments or summaries\n",
    "                                - Do not include any formatting or titles — just the cleaned text\n",
    "                                - Do not paraphrase, interpret, explain or narrate\n",
    "                                - Keep the original phrasing, sentence structure and terminology exactly as written\n",
    "                                \n",
    "                                Here is the original OCR:\"\"\"\n",
    "                        )\n",
    "                else:\n",
    "                    prompt_clean = (\n",
    "                        f\"Ce texte provient d'un OCR d'une page d'un livre ou d'un document scientifique. \"\n",
    "                        f\"Saurais-tu nettoyer le résultat (lettres manquantes, orthographe, mots ou phrases incomplètes, etc.) \"\n",
    "                        f\"tout en modifiant le minimum et en conservant son sens, le tout toujours en {_sTarget_Language} ? \"\n",
    "                        f\"N'ajoute aucun autre commentaire et ne change pas de langue par rapport à la langue cible : \"\n",
    "                    )\n",
    "            \n",
    "                prompt_explain = (\n",
    "                    f\"Ce texte provient d'un OCR d'une page d'un livre ou d'un document scientifique. \"\n",
    "                    f\"Peux-tu reprendre les éléments de la page, les expliquer, annoter et compléter un maximum \"\n",
    "                    f\"tout en conservant le sens original, le tout toujours en {_sTarget_Language} ? \"\n",
    "                    f\"N'ajoute aucun autre commentaire et ne change pas de langue par rapport à la langue cible : \"\n",
    "                )                \n",
    "                \n",
    "                sText_Ollama = Correct_Text_with_Ollama(_sOllamaModel, sText, prompt_clean)\n",
    "                sText_OllamaExp = Correct_Text_with_Ollama(_sOllamaModel, sText, prompt_explain)\n",
    "\n",
    "            if (_bUse_Species_Index):\n",
    "                dSpecies_Result = Extract_Species_From_Page(sText_Ollama, iPage_Num + 1, df_species, family_ranges)\n",
    "                #print (dSpecies_Result)\n",
    "            \n",
    "            if _sSave_Mode == \"SQL\":\n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT INTO Pages (page_book_id_fkey, page_number, page_raw_text, page_ocr_text, page_raw_text_cleaned, page_raw_text_llm, page_raw_text_llm_explain)\n",
    "                    OUTPUT INSERTED.page_id_pkey\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", (iBook_ID, iPage_Num + 1, sText, sOCRText, sTextClean, sText_Ollama, sText_OllamaExp))\n",
    "                page_id = cursor.fetchone()[0]\n",
    "                conn.commit()\n",
    "\n",
    "                for iImageCount, img_base64 in enumerate(oImages):\n",
    "                    img_data = base64.b64decode(img_base64)\n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT INTO Images (image_page_id_fkey, image_data, image_index)\n",
    "                        VALUES (?, ?, ?)\n",
    "                    \"\"\", (page_id, img_data, iImageCount + 1))\n",
    "                conn.commit()\n",
    "\n",
    "                if (_bUse_Species_Index and dSpecies_Result):\n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT INTO Page_Species (\n",
    "                            species_page_id_fkey,\n",
    "                            species_order,\n",
    "                            species_order_common,\n",
    "                            species_family,\n",
    "                            species_genus,\n",
    "                            species_genus_conf,\n",
    "                            species_genus_source,\n",
    "                            species_species,\n",
    "                            species_species_conf,\n",
    "                            species_species_type\n",
    "                        )\n",
    "                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    \"\"\", (\n",
    "                        page_id,\n",
    "                        dSpecies_Result[\"order\"],\n",
    "                        dSpecies_Result[\"order_common\"],\n",
    "                        dSpecies_Result[\"family\"],\n",
    "                        dSpecies_Result[\"genus\"],\n",
    "                        dSpecies_Result[\"genus_conf\"],\n",
    "                        dSpecies_Result[\"genus_source\"],\n",
    "                        dSpecies_Result[\"species\"],\n",
    "                        dSpecies_Result[\"species_conf\"],\n",
    "                        dSpecies_Result[\"species_type\"]\n",
    "                    ))\n",
    "                    conn.commit()                    \n",
    "            else:\n",
    "                # File saving mode\n",
    "                page_folder = os.path.join(output_dir, f\"Page_{iPage_Num+1:03d}\")\n",
    "                os.makedirs(page_folder, exist_ok=True)\n",
    "\n",
    "                with open(os.path.join(page_folder, \"raw_text.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(sText)\n",
    "                with open(os.path.join(page_folder, \"ocr_text.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(sOCRText)\n",
    "                with open(os.path.join(page_folder, \"cleaned_text.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(sTextClean)\n",
    "                if _bUse_Ollama:\n",
    "                    with open(os.path.join(page_folder, \"llm_text.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(sText_Ollama)\n",
    "                    with open(os.path.join(page_folder, \"llm_explanation.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(sText_OllamaExp)\n",
    "\n",
    "                for iImageCount, img_base64 in enumerate(oImages):\n",
    "                    img_data = base64.b64decode(img_base64)\n",
    "                    img_path = os.path.join(page_folder, f\"image_{iImageCount + 1:02d}.jpg\")\n",
    "                    with open(img_path, \"wb\") as img_file:\n",
    "                        img_file.write(img_data)\n",
    "\n",
    "            PageElapsed_Time = round(time.time() - dPage_Start_Time, 3)\n",
    "            print(f\"Page {iPage_Num+1} Processed and Stored in {PageElapsed_Time} seconds (Illustrations Count : {len(oImages)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "    finally:\n",
    "        if _sSave_Mode == \"SQL\":\n",
    "            conn.close()\n",
    "            print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a5fb8-1b96-47a2-958c-e0af7f188e9c",
   "metadata": {},
   "source": [
    "# Process launch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e37da-8bae-4367-8095-60adad9244ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAUNCH THE IMPORT AND CLEANING     #\n",
    "######################################\n",
    "\n",
    "start_time = time.time()\n",
    "print (\"Current Time :\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"\\n \")\n",
    "\n",
    "if _sSave_Mode == \"SQL\":\n",
    "    # Connecting SQL Server\n",
    "    conn = pyodbc.connect(_sConnectString)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "Do_Process_PDF(_sPDF_Path)\n",
    "print(\"\\nDone !\")\n",
    "\n",
    "print (\"\\nCurrent Time :\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "elapsed_time = round(time.time() - start_time, 3)\n",
    "print(f\"\\nCell execution time : {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "339d49a0-8615-4fdd-8f32-e97b68126118",
   "metadata": {},
   "source": [
    "# Debug usage.\n",
    "\n",
    "def Display_Images(oImages):\n",
    "    for iIndex, img_base64 in enumerate(oImages):\n",
    "        img_data = base64.b64decode(img_base64)\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        \n",
    "        # Use of matplotlib\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Illustration {iIndex + 1}\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
